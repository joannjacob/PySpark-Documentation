<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>PySpark CLI Documentation</title>
    <link rel="stylesheet" href="main.css" />
    <link href="https://fonts.googleapis.com/css?family=Chewy" rel="stylesheet" />
</head>
<body>
    <div class="wrap">
            <nav id="navbar">
                <header>
                    <h1>PySpark CLI Documentation</h1>
                </header>
                <ul>
                    <a class="nav-link" href="#Introduction" rel="internal"><li>Introduction</li></a>
                    <a class="nav-link" href="#What_you_should_already_know" rel="internal"><li>What you should already know</li></a>
                    <a class="nav-link" href="#Apache_Spark_and_PySpark" rel="internal"><li>Apache Spark and PySpark</li></a>
                    <a class="nav-link" href="#Installation" rel="internal"><li>Installation </li></a>
                    <a class="nav-link" href="#Available_Commands" rel="internal"><li>Available Commands</li></a>
                    <a class="nav-link" href="#References" rel="internal"><li>References</li></a>
                </ul>
            </nav>
            <main id="main-doc">
            <section class="main-section" id="Introduction">
                <header>Introduction</header>
                <article>
                    <p>This documentation contains the step-by-step procedure to create a PySpark project using a CLI.</p>
                </article>
            </section>
            <section class="main-section" id="What_you_should_already_know">
                <header>What you should already know</header>
                <article>
                    <p>Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.
                      It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.</p>
                    <p>PySpark is the Python API for Spark.</p>
                </article>
            </section>
            <section class="main-section" id="Apache_Spark_and_PySpark">
                <header>Apache Spark and PySpark</header>
                <article>
                    <p>Apache Spark is an open-source distributed general-purpose cluster computing framework with (mostly) in-memory data processing engine that can do ETL, analytics, machine learning and graph processing on large volumes of data at rest (batch processing) or in motion (streaming processing) with rich concise high-level APIs for the programming languages: Scala, Python, Java, R, and SQL. </p>
                    <p>You could also describe Spark as a distributed, data processing engine for batch and streaming modes featuring SQL queries, graph processing, and machine learning.
                        In contrast to Hadoop’s two-stage disk-based MapReduce computation engine, Spark’s multi-stage (mostly) in-memory computing engine allows for running most computations in memory, and hence most of the time provides better performance for certain applications, e.g. iterative algorithms or interactive data mining (read Spark officially sets a new record in large-scale sorting).
                        Spark aims at speed, ease of use, extensibility and interactive analytics.
                        Spark is often called cluster computing engine or simply execution engine.
                        Spark is a distributed platform for executing complex multi-stage applications, like machine learning algorithms, and interactive ad hoc queries. Spark provides an efficient abstraction for in-memory cluster computing called Resilient Distributed Dataset.
                        Using Spark Application Frameworks, Spark simplifies access to machine learning and predictive analytics at scale.
                        Spark is mainly written in Scala, but provides developer API for languages like Java, Python, and R.</p>
                    <p> Why Spark? </p>
                    <li>Easy to Get Started - Spark offers spark-shell that makes for a very easy head start to writing and running Spark applications on the command line on your laptop. You could then use Spark Standalone built-in cluster manager to deploy your Spark applications to a production-grade cluster to run on a full dataset.</li>
                    <li>Unified Engine for Diverse Workloads - Spark combines batch, interactive, and streaming workloads under one rich concise API.

Spark supports near real-time streaming workloads via Spark Streaming application framework.

ETL workloads and Analytics workloads are different, however Spark attempts to offer a unified platform for a wide variety of workloads.

Graph and Machine Learning algorithms are iterative by nature and less saves to disk or transfers over network means better performance.

There is also support for interactive workloads using Spark shell. </li>
                    <li>Leverages the Best in distributed batch data processing - When you think about distributed batch data processing, Hadoop naturally comes to mind as a viable solution. Spark draws many ideas out of Hadoop MapReduce. They work together well - Spark on YARN and HDFS - while improving on the performance and simplicity of the distributed computing engine.
                        For many, Spark is Hadoop++, i.e. MapReduce done in a better way. And it should not come as a surprise, without Hadoop MapReduce (its advances and deficiencies), Spark would not have been born at all.</li>
                    <li>Interactive Exploration / Exploratory Analytics - It is also called ad hoc queries. Using the Spark shell you can execute computations to process large amount of data (The Big Data). It’s all interactive and very useful to explore the data before final production release.</li>
                    <li>RDD - Distributed Parallel Scala Collections
As a Scala developer, you may find Spark’s RDD API very similar (if not identical) to Scala’s Collections API.

It is also exposed in Java, Python and R (as well as SQL, i.e. SparkSQL, in a sense).</li>
                    <li>Rich Standard Library -
Not only can you use map and reduce (as in Hadoop MapReduce jobs) in Spark, but also a vast array of other higher-level operators to ease your Spark queries and application development.

It expanded on the available computation styles beyond the only map-and-reduce available in Hadoop MapReduce.</li>
                    <li>Unified development and deployment environment for all -
Regardless of the Spark tools you use - the Spark API for the many programming languages supported - Scala, Java, Python, R, or the Spark shell, or the many Spark Application Frameworks leveraging the concept of RDD, i.e. Spark SQL, Spark Streaming, Spark MLlib and Spark GraphX, you still use the same development and deployment environment to for large data sets to yield a result, be it a prediction (Spark MLlib), a structured data queries (Spark SQL) or just a large distributed batch (Spark Core) or streaming (Spark Streaming) computation.</li>
                    <li>Single Environment -
Regardless of which programming language you are good at, be it Scala, Java, Python, R or SQL, you can use the same single clustered runtime environment for prototyping, ad hoc queries, and deploying your applications leveraging the many ingestion data points offered by the Spark platform.</li>
                    <li>Data Integration Toolkit with Rich Set of Supported Data Sources -
Spark can read from many types of data sources — relational, NoSQL, file systems, etc. — using many types of data formats - Parquet, Avro, CSV, JSON.</li>
                    <li>Tools unavailable then, at your fingertips now - Spark embraces many concepts in a single unified development and runtime environment.

Machine learning that is so tool- and feature-rich in Python, e.g. SciKit library, can now be used by Scala developers (as Pipeline API in Spark MLlib or calling pipe()).

DataFrames from R are available in Scala, Java, Python, R APIs.

Single node computations in machine learning algorithms are migrated to their distributed versions in Spark MLlib.</li>
                    <li>Low-level Optimizations -
Apache Spark uses a directed acyclic graph (DAG) of computation stages (aka execution DAG). It postpones any processing until really required for actions. Spark’s lazy evaluation gives plenty of opportunities to induce low-level optimizations (so users have to know less to do more).</li>
                    <li>Excels at low-latency iterative workloads -
Spark supports diverse workloads, but successfully targets low-latency iterative ones. They are often used in Machine Learning and graph algorithms.</li>
                    <li>ETL done easier -
Spark gives Extract, Transform and Load (ETL) a new look with the many programming languages supported - Scala, Java, Python (less likely R). You can use them all or pick the best for a problem.

Scala in Spark, especially, makes for a much less boiler-plate code (comparing to other languages and approaches like MapReduce in Java).</li>
                    <li>Unified Concise High-Level API -
Spark offers a unified, concise, high-level APIs for batch analytics (RDD API), SQL queries (Dataset API), real-time analysis (DStream API), machine learning (ML Pipeline API) and graph processing (Graph API).</li>
                    <li>Different kinds of data processing using unified API -
Spark offers three kinds of data processing using batch, interactive, and stream processing with the unified API and data structures.</li>
                    <li>Little to no disk use for better performance</li>
                    <li>Fault Tolerance included</li>
                    <li>Small Codebase Invites Contributors</li>
                </article>
            </section>
            <section class="main-section" id="Installation">
                <header>Installation</header>

                <p>Follow these steps for installation on Ubuntu. For installation in Windows, follow the link <a href="https://medium.com/big-data-engineering/how-to-install-apache-spark-2-x-in-your-pc-e2047246ffc3">How to Install Apache Spark on Windows</a></p>
              <p>1. Download and Install JDK 8 or above.
                  Before you can start with spark and hadoop, you need to make sure you have java 8 installed, or to install it.
                 </p>
                <article>
                    You should check Java by running following command::
                    <code> java -version
                    </code>
                    Select the code in the pad and hit Ctrl+R to watch it unfold in your browser!
                </article>
                <p>If JDK 8 is not installed you should follow the tutorial <a href="https://www.roseindia.net/answers/viewqa/linux/32404-How-to-Install-Oracle-Java-JDK-8-in-Ubuntu-16-04-.html">How to Install Oracle Java JDK 8 in Ubuntu 16.04?</a></p>
                <p>2. Download and install Apache Spark
                    Now the next step is to download latest distribution of Spark. Visit the website <a href="https://spark.apache.org/downloads.html">https://spark.apache.org/downloads.html</a> and there you will find the latest distribution of Spark framework. </p>
                    <article>Create a directory spark with following command in your home.
                <code>mkdir spark</code>
                    </article>
                <article>Move spark-2.3.0-bin-hadoop2.7.tgz in the spark directory:
                <code>mv ~/Downloads/spark-2.3.0-bin-hadoop2.7.tgz spark
                    cd spark/
                    tar -xzvf spark-2.3.0-bin-hadoop2.7.tgz
                </code>
                    After extracting the file go to bin directory of spark and run ./pyspark. 
                    It will open following pyspark shell:
                    <p>
                    <img src="resources/images/pyspark_shell.jpg" alt = "pyspark_shell" height = "250" width = "250" />
                    </p>
                    </article>
                <p>3. Configure Apache Spark
                    Now you should configure it in path so that it can be executed from anywhere. </p>
                <article>
                    Open bash_profile file:
                    <code> vi ~/.bash_profile
                    </code>
                    Add following entry:
                    <code>
                        export SPARK_HOME=~/spark/spark-2.3.0-bin-hadoop2.7/
                        export PATH="$SPARK_HOME/bin:$PATH"
                    </code>
                    Run the following command to update PATH variable in the current session:
                    <code>
                        source ~/.bash_profile
                    </code>
                    After next login you should be able to find pyspark command in path and it can be accessed from any directory.
                </article>
                <p>4. Check PySpark installation </p>
                <p><article>
                    In your anaconda prompt,or any python supporting cmd, type pyspark, to enter pyspark shell. To be prepared, best to check it in the python environment from which you run jupyter notebook.
                    You are supposed to see the following:</article>
                <img src="resources/images/pyspark_shell.jpg" alt = "pyspark_shell" height = "250" width = "250" />

                <article>Run the following commands, the output should be [1,4,9,16].
                    <code>cmd> pyspark
                        >>> nums = sc.parallelize([1,2,3,4])
                        >>> nums.map(lambda x: x*x).collect()
                    </code>
                    To exit pyspark shell, type Ctrl-z and enter. Or the python command exit()
                </article></p>
            </section>
            <section class="main-section" id="Available_Commands">
                <header>Available Commands</header>

            </section>
            <section class="main-section" id="References">
                <header>Reference</header>
                <article>
                  <p>All the documentation in this page is taken from: </p>
                      <li><a href="https://spark.apache.org/docs/latest/" target="_blank">Apache Spark</a></li>
                  <li><a href="https://spark.apache.org/docs/latest/api/python/pyspark.html" target="_blank">PySpark</a></li>
                  <li><a href="https://spark.apache.org/downloads.html" target="_blank">Download Apache Spark</a></li>
                </article>
            </section>
        </main>
    </div>
    <footer>
    <a href="https://www.disegnosis.com.ar" target="_blank">
        <img src="https://www.diseñowebs.com.ar/freecodecamp/responsive-web-design-projects/tribute-page/img/disegnosis.png" alt="Diseño Web DISEGNOSIS - Webmaster Diseño de Páginas / Sitios Web. Servicios de Hosting, Streaming y Mailing." />
    </a>
    </footer>
</body>
</html>

